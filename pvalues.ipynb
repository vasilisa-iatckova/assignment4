{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Part 2 - Effect size, power, and p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the libraries you will use, notice we will be calling `numpy` with `np.` notation, and `matplotlib.pyplot` as `plt`. These are conventional ways to call these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with synthetic data\n",
    "\n",
    "First, it is important to know that most random number generators are pseudo-random. Meaning, the random numbers are generated with a seed or initial value, and if you declare this seed at the beginning of your analysis, your randomly generated numbers will be consistent every time your code is ran, giving you reproducible results. In `numpy`'s `random` module, you can declare your random number generator with `seed`, see this answer here for an example: https://stackoverflow.com/a/21494630\n",
    "\n",
    "There are numerous ways to generate random data, you can check out all the `numpy.random` capabilities here: https://numpy.org/doc/1.16/reference/routines.random.html\n",
    "\n",
    "For now, we will start with the standard normally distributed data (Gaussians). Start by generating 20 random data points with `numpy`'s `randn`, and visualizing it as a histogram with `matplotlib`'s `hist`: https://matplotlib.org/3.3.4/api/_as_gen/matplotlib.pyplot.hist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random number generator initial seed/value:\n",
    "\n",
    "\n",
    "# Create a variable that has 20 normally distributed data points. (Use numpy.random.randn)\n",
    "data = \n",
    "\n",
    "# Visualize the data you just created with `plt.hist()`. You can tell the histogram to display bin sizes to your liking with `plt.hist(data, bins=#)`. Play around with bin sizes until a decent looking histogram appears.\n",
    "figure1, axes1 = plt.subplots()     # initialize a figure and axes for drawing\n",
    "axes1.hist(data, bins = 5)          # Input your data and bins\n",
    "axes1.set_title('Histogram of Synthetic Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal (Gaussian) distribution is sufficiently defined by 2 statistics, it's mean and variance. `numpy` has a `mean()` and a `std()` function to compute the mean and standard deviation of your data. Compute the mean and standard deviation of your dataset and use the `print()` function to display the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean: \n",
    "data_mean = \n",
    "\n",
    "# compute the variance: \n",
    "data_std = \n",
    "\n",
    "print('Data mean is ', data_mean)\n",
    "print('Data standard deviation is ', data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you carefully read `randn`'s documentation, you should have known that `randn` samples from a \"standard\" normal distribution, which has a mean = 0 and standard deviation of 1. Does your `data`'s mean and standard deviation match this expectation of a \"standard\" normal distribution? If not, why not?\n",
    "\n",
    " > Answer in this markdown quotation block (`>`) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of 20 data points, replace `data` with 1000 data points from `randn`, display the histogram and compute the mean and variances again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed your Random Number Generator (RNG)\n",
    "\n",
    "# Replace data with 1000 points:\n",
    "data = \n",
    "\n",
    "# compute and mean and variances, print the answer out like before:\n",
    "new_mean = \n",
    "new_std = \n",
    "\n",
    "# display the histogram with matplotlib\n",
    "\n",
    "# Does the mean and variances match now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more realistic scenario.\n",
    "\n",
    "Now, suppose you are recording the firing rate of neurons pre-treatment and post-treatment. Let's say your treatment is hypothesized to increase the firing rate of neurons. \n",
    "\n",
    "From your pilot study, you recorded from a small field of view that has 10 neurons, and your recording software summarized the recording and told you:\n",
    "\n",
    " - N = 10 neurons \n",
    " - Pre-treatment data firing rate per second: mean = 150\n",
    " - Post-treatment data firing rate per second: mean = 160\n",
    " - Both datasets have about the same standard deviation $\\sigma^2=30$, and assume normal distribution.\n",
    "\n",
    "Use `np.random.normal()` to generate your 2 datasets, see docs here: https://numpy.org/doc/1.16/reference/generated/numpy.random.normal.html#numpy.random.normal\n",
    "\n",
    " - `np.random.normal` draws samples from a normal distribution, the first input `loc` is the mean of the distribution, the second input `scale` is the standard deviation, and the last is the amount of samples you want to draw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 fake datasets according to the pilot study scenario above, do not use a `seed` this time, also no need to plot the histogram:\n",
    "N  = 10\n",
    "s2 = 30     # standard deviation is the same for both groups\n",
    "\n",
    "pre_data  =\n",
    "post_data ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing this data, you got very excited and wanted to see if you got your hands on a significant finding, Perform a 2-sampled t-test with `scipy.stats.ttest_ind`, look at the documentations with `?` and plug in your data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Read scipy's t-test implementation docs:\n",
    "ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the t-statistics and obtain your p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the t-test:\n",
    "results = ttest_ind()   # plug in your data\n",
    "\n",
    "# print output with descriptions:\n",
    "print(\"My t-statistic is \", results[0])\n",
    "print(\"My p-value is \", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't use a `seed` this time, you can feel free and re-run the data generation cells and the t-test to see how your results change when different random samples are collected.\n",
    "\n",
    "Should you be using the current output of a 2-tailed p-value for this experiment? Or should you be using a one-tailed p-value? What is your estimated one-tailed p-value? \n",
    "\n",
    " > Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going by the popular stance of $p<0.05$ being significant, is your current t-test's result significant? What are the contributing factors to your p-value? \n",
    " > Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You think that there should be a treatment effect in your current data, so you googled around and found Cohen's D as a metric for effect size, the formula being:\n",
    "\n",
    "$$\n",
    "d = \\frac{u_1 - u_2}{s}\n",
    "$$\n",
    "\n",
    "Where $s$ is the pooled standard deviation for two independent samples:\n",
    "\n",
    "$$\n",
    "s = \\sqrt{ \\frac{SD_1^2 + SD_2^2}{2}}\n",
    "$$\n",
    "\n",
    "With $SD_1$ and $SD_2$ being standard deviations for the two samples respectively.\n",
    "\n",
    "So you set out to compute this metric with the following readily made function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(a, b):\n",
    "    \"\"\"\n",
    "    Compute effect size with Cohen's D for samples a and b. \n",
    "    \"\"\"\n",
    "\t# calculate the means of the sample\n",
    "    u1 = np.mean(a)\n",
    "    u2 = np.mean(b)\n",
    "    # calculate the simplified pooled variance:\n",
    "    s = np.sqrt((np.std(a)**2 + np.std(b)**2)/2)\n",
    "    return (u1 - u2)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above function to compute your effect size:\n",
    "d = cohen_d()\n",
    "\n",
    "print(\"My current effect size is \" d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your current effect size large? Use the [Cohen's D Wikipedia page](https://en.wikipedia.org/wiki/Effect_size#Cohen's_d) to judge if you are not too familiar with the metric. What does your $p$-value and $d$ metric say about your current results? Are they reliable? \n",
    " \n",
    " > Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the above experiment with $N=40$ while everything else is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40      # New sample size\n",
    "\n",
    "# Generate your data:\n",
    "\n",
    "# Compute the T-statistic:\n",
    "\n",
    "# Compute the Cohen's D:\n",
    "\n",
    "# Print out your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should re-execute the above code cell for multiple trials of your experiment, you should be seeing a lot of different results because the variance in your data. \n",
    "\n",
    "What conclusions can you draw about the relationship of sample size and $p$-value? Why should you report effect size in addition to $p$-values?\n",
    " \n",
    " > Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Hacking:\n",
    "\n",
    "P-hacking is where scientists try to achieve a significant result by increasing the sample size. In the following for-loop, we are dealing with data that's normally distributed with a small effect size (variance of 2, but difference in mean is only 1). \n",
    "\n",
    "Run this code with different sample sizes (N), and see how the output changes, don't be shy with N, you can go crazy high, but something as high as 500,000 might take a minute or two to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "count_significant = 0   # start with 0 count\n",
    "ntrials = 1000          # repeat the loop 1000 times\n",
    "for i in range(ntrials):\n",
    "    # Generate data\n",
    "    a = np.random.normal(1,2,N)\n",
    "    b = np.random.normal(0,2,N)\n",
    "\n",
    "    # Compute statistics\n",
    "    result = ttest_ind(a,b)\n",
    "\n",
    "    # If pvalue < 0.05, count it.\n",
    "    if result[1] < 0.05:\n",
    "        count_significant += 1\n",
    "\n",
    "print('The t-test was significant {}% of the time'.format((count_significant/ntrials)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-value is supposed to indicate your chances of getting a false positive result. At what sample size (N) did you start seeing significant results more than 5% of the time? 20%? 50%? ~90%? \n",
    "\n",
    " > Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
